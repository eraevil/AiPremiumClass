{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97792e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d360709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_size, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_size, dropout=dropout, batch_first=True, bidirectional=True)\n",
    "    def forward(self, token_seq, mode='concat'):\n",
    "        embedded = self.embedding(token_seq)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        if mode == 'concat':\n",
    "            return torch.cat((hidden[0], hidden[1]), dim=1), outputs\n",
    "        else:\n",
    "            return hidden[0] + hidden[1] ,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eac92539",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, enc_output, dec_output):\n",
    "        # a_t = h_t @ h_s\n",
    "        a_t = torch.bmm(enc_output, dec_output.permute(0, 2, 1))\n",
    "        a_t = torch.softmax(a_t, dim=1)\n",
    "        c_t = torch.bmm(a_t.permute(0, 2, 1), enc_output)\n",
    "        return c_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74d5f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, dropout, mode='concat'):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 定义嵌入层\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        if mode=='add':\n",
    "            self.hidden_up = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "            self.rnn = nn.GRU(emb_dim, hidden_dim , dropout=dropout,\n",
    "                              batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "            self.atteniton_fc = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        else:\n",
    "            self.rnn = nn.GRU(emb_dim, hidden_dim * 2, dropout=dropout,\n",
    "                            batch_first=True)\n",
    "            self.fc = nn.Linear(hidden_dim * 2, input_dim)\n",
    "            self.atteniton_fc = nn.Linear(hidden_dim * 4, hidden_dim * 2)\n",
    "            self.hidden_up = nn.Identity()\n",
    "        self.atteniton = Attention()\n",
    "\n",
    "    \n",
    "    def forward(self, token_seq, hidden_state, enc_output):\n",
    "        # print(hidden_state.unsqueeze(0).shape)\n",
    "        embedded = self.embedding(token_seq)\n",
    "        # print(embedded.shape)\n",
    "        dec_output, hidden = self.rnn(embedded, hidden_state.unsqueeze(0))\n",
    "        dec_output = self.hidden_up(dec_output)\n",
    "        # print(dec_output.shape)\n",
    "        # print(hidden.shape)\n",
    "        # print(enc_output.shape)\n",
    "        c_t = self.atteniton(enc_output, dec_output)\n",
    "        dec_output = self.atteniton_fc(torch.cat((dec_output, c_t), dim=-1))\n",
    "        out = torch.tanh(dec_output) # 激活\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "971f8e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,\n",
    "                 enc_emb_size,\n",
    "                 dec_emb_size,\n",
    "                 emb_dim,\n",
    "                 hidden_size,\n",
    "                 dropout=0.5,\n",
    "                 mode='concat'\n",
    "                 ):\n",
    "\n",
    "        super().__init__()\n",
    "        # encoder\n",
    "        self.encoder = Encoder(enc_emb_size, emb_dim, hidden_size, dropout=dropout)\n",
    "        self.decoder = Decoder(dec_emb_size, emb_dim, hidden_size, dropout=dropout, mode=mode)\n",
    "\n",
    "    def forward(self, enc_input, dec_input, mode='concat'):\n",
    "        # encoder last hidden state\n",
    "        encoder_state, outputs = self.encoder(enc_input, mode=mode)\n",
    "        output, hidden = self.decoder(dec_input, encoder_state, outputs)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30775133",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:\\\\study\\\\AI\\\\data\\\\chinese-couplets\\\\versions\\\\2\\\\couplet\\\\vocabs\", encoding=\"utf-8\") as f:\n",
    "    vocab_list = [line.strip() for line in f]\n",
    "\n",
    "vocab_list = [\"PAD\", \"UNK\"] + vocab_list\n",
    "# 字符->索引\n",
    "evoc = {char : idx for idx, char in enumerate(vocab_list)}\n",
    "# 索引->字符\n",
    "dvoc = {idx : char for char, idx in evoc.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "136c690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_proc(evoc, dvoc):\n",
    "    def collate_fn(batch):\n",
    "        enc_seqs, dec_seqs = zip(*batch) # encoder输入和decoder输出\n",
    "        # 把字符序列转换为索引序列\n",
    "        enc_idx = [torch.tensor([evoc.get(c, evoc['UNK'] ) for c in seq]) for seq in enc_seqs]\n",
    "        # 解码输入添加 <s> 起始符，解码目标添加 </s> 终止符\n",
    "        dec_idx = [torch.tensor(\n",
    "            [evoc['<s>']] + [dvoc.get(c, evoc['UNK']) for c in seq] + [evoc['</s>']]\n",
    "        ) for seq in dec_seqs]\n",
    "        # 填充序列\n",
    "        enc_padded = pad_sequence(enc_idx, batch_first=True, padding_value=evoc['PAD'])\n",
    "        dec_padded = pad_sequence(dec_idx, batch_first=True, padding_value=evoc['PAD'])\n",
    "        targets = [seq[1:] for seq in dec_padded] # 目标去掉开头<S>\n",
    "        targets_padded = pad_sequence(targets, batch_first=True, padding_value=evoc['PAD'])\n",
    "        return enc_padded, dec_padded[:, :-1], targets_padded\n",
    "    return collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5cc17d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, loss_fn, optimizer, writer, train_loader, epochs=20, mode='concat',name='seq2seq_cat'):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for i, (enc_x, dec_x, target) in enumerate(tqdm(train_loader)):\n",
    "            enc_x, dec_x, target = enc_x.cuda(), dec_x.cuda(), target.cuda()\n",
    "            logits, _ = model(enc_x, dec_x, mode=mode)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), target.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            writer.add_scalar('train/loss', loss.item(), epoch * len(train_loader) + i)\n",
    "        print(f'epoch: {epoch}, loss: {total_loss / len(train_loader)}')\n",
    "    torch.save(model.state_dict(), f'{name}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "778183e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"E:\\\\study\\\\AI\\\\data\\\\chinese-couplets\\\\versions\\\\2\\\\couplet\\\\train\\\\in.txt\", encoding=\"utf-8\") as f:\n",
    "    inputs = [line.strip() for line in f]\n",
    "with open(\"E:\\\\study\\\\AI\\\\data\\\\chinese-couplets\\\\versions\\\\2\\\\couplet\\\\train\\\\out.txt\", encoding=\"utf-8\") as f:\n",
    "    outputs = [line.strip() for line in f]\n",
    "\n",
    "train_set = list(zip(inputs, outputs))\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True, collate_fn=get_proc(evoc, dvoc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64716852",
   "metadata": {},
   "source": [
    "# test cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a32542d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = Seq2Seq(len(vocab_list), len(vocab_list), emb_dim=128, hidden_size=128, dropout=0.5).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=evoc['PAD'])\n",
    "writer = SummaryWriter(log_dir=\"homework\\\\week08\\\\cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30c29853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [04:03<00:00, 49.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.045530947104281065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [03:56<00:00, 50.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.012563692718363944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [03:58<00:00, 50.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.008558550638696177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [04:01<00:00, 49.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.007094684156260348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [03:59<00:00, 50.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 0.006563768271730396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "process(model, loss_fn, optimizer, writer, train_loader, epochs=5, mode='concat', name='seq2seq_cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "76f5a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3021]], device='cuda:0')\n",
      "tensor([[4739]], device='cuda:0')\n",
      "tensor([[8507]], device='cuda:0')\n",
      "tensor([[4119]], device='cuda:0')\n",
      "tensor([[6593]], device='cuda:0')\n",
      "tensor([[616]], device='cuda:0')\n",
      "tensor([[2298]], device='cuda:0')\n",
      "沛褥摠傩尻鸡磊\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morry\\AppData\\Local\\Temp\\ipykernel_32944\\2364540773.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_cat.load_state_dict(torch.load(\"seq2seq_cat.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model_cat = Seq2Seq(\n",
    "    enc_emb_size=len(evoc),\n",
    "    dec_emb_size=len(evoc),\n",
    "    emb_dim=128, \n",
    "    hidden_size=128,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "model_cat.load_state_dict(torch.load(\"seq2seq_cat.pth\"))\n",
    "upper = \"无花无酒无花酒\"\n",
    "max_dec_len = 7\n",
    "dec_tokens= []\n",
    "dec_input = torch.tensor([[evoc['<s>']]]).to(device)\n",
    "model_cat.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    enc_input = torch.tensor([[evoc.get(c, evoc['UNK']) for c in upper]]).to(device)\n",
    "    enc_hidden, enc_output  = model_cat.encoder(enc_input, mode='concat')\n",
    "\n",
    "    while True:\n",
    "        if len(dec_tokens) >= max_dec_len:\n",
    "            break\n",
    "        # print(enc_input.shape)\n",
    "        # print(enc_output.shape, hidden.shape)\n",
    "        logits, hidden = model_cat.decoder(dec_input, enc_hidden, enc_output)\n",
    "        predicted_idx = torch.argmax(logits, dim=-1)\n",
    "        print(predicted_idx)\n",
    "        if dvoc[predicted_idx.squeeze().item()] == '</s>':\n",
    "            break\n",
    "        dec_tokens.append(predicted_idx.squeeze().item())\n",
    "        dec_input = predicted_idx\n",
    "    \n",
    "print(''.join([dvoc[idx] for idx in dec_tokens]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d726337",
   "metadata": {},
   "source": [
    "# test add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fedf653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [03:04<00:00, 65.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 0.20677100896630482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [03:02<00:00, 66.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 0.029715389054948117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [03:04<00:00, 65.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2, loss: 0.024766155037593735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [02:59<00:00, 66.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3, loss: 0.02338384183167454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [02:55<00:00, 68.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4, loss: 0.025448132585351203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [02:54<00:00, 69.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5, loss: 0.019867335473235152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [02:54<00:00, 69.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6, loss: 0.025008262674501677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [02:54<00:00, 68.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7, loss: 0.02195845811163227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [02:52<00:00, 69.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8, loss: 0.024769679226888035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12039/12039 [02:55<00:00, 68.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9, loss: 0.02127216277900947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model = Seq2Seq(len(vocab_list), len(vocab_list), emb_dim=128, hidden_size=128, dropout=0.5, mode='add').to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=evoc['PAD'])\n",
    "writer = SummaryWriter(log_dir=\"homework\\\\week08\\\\add\")\n",
    "process(model, loss_fn, optimizer, writer, train_loader, epochs=10, mode='add', name=\"Seq2Seq_add\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2963f850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1]], device='cuda:0')\n",
      "tensor([[3]], device='cuda:0')\n",
      "UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morry\\AppData\\Local\\Temp\\ipykernel_32944\\4087851060.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_add.load_state_dict(torch.load(\"Seq2Seq_add.pth\"))\n"
     ]
    }
   ],
   "source": [
    "model_add = Seq2Seq(\n",
    "    enc_emb_size=len(evoc),\n",
    "    dec_emb_size=len(evoc),\n",
    "    emb_dim=128, \n",
    "    hidden_size=128,\n",
    "    dropout=0.5,\n",
    "    mode='add'\n",
    ").to(device)\n",
    "model_add.load_state_dict(torch.load(\"Seq2Seq_add.pth\"))\n",
    "upper = \"无花无酒无花酒\"\n",
    "max_dec_len = 7\n",
    "dec_tokens= []\n",
    "dec_input = torch.tensor([[evoc['<s>']]]).to(device)\n",
    "\n",
    "while True:\n",
    "    if len(dec_tokens) >= max_dec_len:\n",
    "        break\n",
    "    model_add.eval()\n",
    "    with torch.no_grad():\n",
    "        enc_input = torch.tensor([[evoc.get(c, evoc['UNK']) for c in upper]]).to(device)\n",
    "        # print(enc_input.shape)\n",
    "        enc_output, hidden = model_add.encoder(enc_input, mode='add')\n",
    "        # print(enc_output.shape, hidden.shape)\n",
    "        logits, hidden = model_add.decoder(dec_input, enc_output, hidden)\n",
    "        predicted_idx = torch.argmax(logits, dim=-1)\n",
    "        print(predicted_idx)\n",
    "        if dvoc[predicted_idx.squeeze().item()] == '</s>':\n",
    "            break\n",
    "        dec_tokens.append(predicted_idx.squeeze().item())\n",
    "        dec_input = predicted_idx\n",
    "    \n",
    "print(''.join([dvoc[idx] for idx in dec_tokens]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
