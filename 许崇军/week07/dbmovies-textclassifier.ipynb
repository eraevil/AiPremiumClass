{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1683,"sourceType":"datasetVersion","datasetId":600}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:01:27.186371Z","iopub.execute_input":"2025-04-17T07:01:27.186710Z","iopub.status.idle":"2025-04-17T07:01:27.864187Z","shell.execute_reply.started":"2025-04-17T07:01:27.186689Z","shell.execute_reply":"2025-04-17T07:01:27.863451Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/doubanmovieshortcomments/DMSC.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pickle\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.nn.utils.rnn import pad_sequence\nimport jieba\nimport pandas as pd\nfrom collections import Counter\nimport logging\n\njieba.setLogLevel(logging.INFO)\n\n\n# # 加载数据\n# def load_data(file_path):\n#     df = pd.read_csv(file_path)\n#     data = []\n#     for index, row in df.iterrows():\n#         score = row['Star']\n#         if 1 <= score <= 2: \n#             label = 1\n#         elif 4 <= score <= 5:\n#             label = 0\n#         else:\n#             continue  # 跳过不符合条件的行\n#         comment = row['Comment']\n#         data.append((comment, label))\n#     return data  # 返回包含评论和标签的列表\n\n\n# 构建词汇表\ndef build_from_doc(doc):\n    vocab = set()\n    for line in doc:\n        words = jieba.lcut(line[0])\n        vocab.update(words)\n    vocab = ['PAD', 'UNK'] + list(vocab)  # PAD: padding, UNK: unknown\n    w2idx = {word: idx for idx, word in enumerate(vocab)}\n    return w2idx\n\n\n# 自定义数据集类\nclass DoubanDataset(Dataset):\n    def __init__(self, data, vocab):\n        self.data = data\n        self.vocab = vocab\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        comment, label = self.data[idx]\n        words = jieba.lcut(comment)\n        comment_idx = [self.vocab.get(word, self.vocab['UNK']) for word in words]\n        return torch.tensor(comment_idx), torch.tensor(label)\n\n\nclass Comments_Classifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # padding_idx=0\n        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, input_ids):\n        # input_ids: (batch_size, seq_len)\n        # embedded: (batch_size, seq_len, embedding_dim)\n        embedded = self.embedding(input_ids)\n        # output: (batch_size, seq_len, hidden_size)\n        output, (hidden, _) = self.rnn(embedded)\n        output = self.fc(output[:, -1, :])  # 取最后一个时间步的输出\n        return output\n\n\n# 自定义数据转换方法(callback function)回调函数\ndef convert_data(batch_data):\n    comments, votes = [], []\n    # 分别提取评论和标签\n    for comment, vote in batch_data:\n        comments.append(comment)\n        votes.append(vote)\n\n    # 将评论和标签转换为tensor\n    commt = pad_sequence(comments, batch_first=True, padding_value=0)  # 填充为相同长度\n    labels = torch.tensor(votes)\n    # 返回评论和标签\n    return commt, labels","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:25:28.985790Z","iopub.execute_input":"2025-04-17T07:25:28.986163Z","iopub.status.idle":"2025-04-17T07:25:28.997306Z","shell.execute_reply.started":"2025-04-17T07:25:28.986140Z","shell.execute_reply":"2025-04-17T07:25:28.996595Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # 加载数据\nfile_path = '/kaggle/input/doubanmovieshortcomments/DMSC.csv'\ndf = pd.read_csv(file_path)\ndf = df.query('(1 <= Star <= 2) or (4 <= Star <= 5)')\ndef get_label(score):\n    if 1 <= score <= 2:\n        return 1\n    elif 4 <= score <= 5:\n        return 0\ndf['label'] = df['Star'].apply(get_label)\ndata = list(zip(df['Comment'], df['label']))\n\n    # 构建词汇表\nvocab = build_from_doc(data)\n    # print('词汇表大小:', len(vocab))\n\n    # 划分训练集和测试集\ntrain_size = int(0.8 * len(data))\ntrain_data = data[:train_size]\ntest_data = data[train_size:]\n\n    # 创建数据集和数据加载器\ntrain_dataset = DoubanDataset(train_data, vocab)\ntest_dataset = DoubanDataset(test_data, vocab)\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=convert_data)\ntest_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=convert_data)\n\n    # 构建模型\nvocab_size = len(vocab)\nembedding_dim = 100\nhidden_size = 128\nnum_classes = 2\n\nmodel = Comments_Classifier(vocab_size, embedding_dim, hidden_size, num_classes)\nmodel.to(device)\n\n    # 定义损失函数和优化器\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n    # 训练模型\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    for i, (cmt, lbl) in enumerate(train_dataloader):\n        cmt = cmt.to(device)\n        lbl = lbl.to(device)\n\n            # 前向传播\n        outputs = model(cmt)\n        loss = criterion(outputs, lbl)\n\n            # 反向传播和优化\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if (i + 1) % 1000 == 0:\n            print(\n             f'Epoch [{epoch + 1}/{num_epochs}],  Loss: {loss.item():.4f}')\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:25:33.038308Z","iopub.execute_input":"2025-04-17T07:25:33.038910Z","iopub.status.idle":"2025-04-17T07:56:47.481457Z","shell.execute_reply.started":"2025-04-17T07:25:33.038884Z","shell.execute_reply":"2025-04-17T07:56:47.480643Z"}},"outputs":[{"name":"stdout","text":"Epoch [1/5],  Loss: 0.5673\nEpoch [1/5],  Loss: 0.5299\nEpoch [1/5],  Loss: 0.5335\nEpoch [1/5],  Loss: 0.5959\nEpoch [1/5],  Loss: 0.6433\nEpoch [1/5],  Loss: 0.5713\nEpoch [1/5],  Loss: 0.5955\nEpoch [1/5],  Loss: 0.3625\nEpoch [1/5],  Loss: 0.5750\nEpoch [1/5],  Loss: 0.3826\nEpoch [1/5],  Loss: 0.4110\nEpoch [1/5],  Loss: 0.2938\nEpoch [1/5],  Loss: 0.0807\nEpoch [1/5],  Loss: 0.3066\nEpoch [1/5],  Loss: 0.2144\nEpoch [1/5],  Loss: 0.6388\nEpoch [1/5],  Loss: 0.2592\nEpoch [1/5],  Loss: 0.1397\nEpoch [1/5],  Loss: 0.4355\nEpoch [1/5],  Loss: 0.3093\nEpoch [1/5],  Loss: 0.1921\nEpoch [1/5],  Loss: 0.3916\nEpoch [1/5],  Loss: 0.3069\nEpoch [1/5],  Loss: 0.2943\nEpoch [1/5],  Loss: 0.1633\nEpoch [1/5],  Loss: 0.2365\nEpoch [1/5],  Loss: 0.1080\nEpoch [1/5],  Loss: 0.3632\nEpoch [1/5],  Loss: 0.4363\nEpoch [1/5],  Loss: 0.2564\nEpoch [1/5],  Loss: 0.1185\nEpoch [1/5],  Loss: 0.2570\nEpoch [1/5],  Loss: 0.2397\nEpoch [1/5],  Loss: 0.2001\nEpoch [1/5],  Loss: 0.4449\nEpoch [1/5],  Loss: 0.3276\nEpoch [1/5],  Loss: 0.2500\nEpoch [1/5],  Loss: 0.2114\nEpoch [1/5],  Loss: 0.0948\nEpoch [1/5],  Loss: 0.0827\nEpoch [1/5],  Loss: 0.2812\nEpoch [2/5],  Loss: 0.3204\nEpoch [2/5],  Loss: 0.2318\nEpoch [2/5],  Loss: 0.2509\nEpoch [2/5],  Loss: 0.0931\nEpoch [2/5],  Loss: 0.1321\nEpoch [2/5],  Loss: 0.4650\nEpoch [2/5],  Loss: 0.2217\nEpoch [2/5],  Loss: 0.0920\nEpoch [2/5],  Loss: 0.0780\nEpoch [2/5],  Loss: 0.0968\nEpoch [2/5],  Loss: 0.1820\nEpoch [2/5],  Loss: 0.2999\nEpoch [2/5],  Loss: 0.1377\nEpoch [2/5],  Loss: 0.1477\nEpoch [2/5],  Loss: 0.2108\nEpoch [2/5],  Loss: 0.2328\nEpoch [2/5],  Loss: 0.1440\nEpoch [2/5],  Loss: 0.1874\nEpoch [2/5],  Loss: 0.3693\nEpoch [2/5],  Loss: 0.3535\nEpoch [2/5],  Loss: 0.1288\nEpoch [2/5],  Loss: 0.1408\nEpoch [2/5],  Loss: 0.2207\nEpoch [2/5],  Loss: 0.1084\nEpoch [2/5],  Loss: 0.1348\nEpoch [2/5],  Loss: 0.2237\nEpoch [2/5],  Loss: 0.2869\nEpoch [2/5],  Loss: 0.3971\nEpoch [2/5],  Loss: 0.0815\nEpoch [2/5],  Loss: 0.2904\nEpoch [2/5],  Loss: 0.3151\nEpoch [2/5],  Loss: 0.1063\nEpoch [2/5],  Loss: 0.2287\nEpoch [2/5],  Loss: 0.1660\nEpoch [2/5],  Loss: 0.1104\nEpoch [2/5],  Loss: 0.1168\nEpoch [2/5],  Loss: 0.2246\nEpoch [2/5],  Loss: 0.1357\nEpoch [2/5],  Loss: 0.3064\nEpoch [2/5],  Loss: 0.2078\nEpoch [2/5],  Loss: 0.2201\nEpoch [3/5],  Loss: 0.4836\nEpoch [3/5],  Loss: 0.2000\nEpoch [3/5],  Loss: 0.1626\nEpoch [3/5],  Loss: 0.1232\nEpoch [3/5],  Loss: 0.1407\nEpoch [3/5],  Loss: 0.1530\nEpoch [3/5],  Loss: 0.0892\nEpoch [3/5],  Loss: 0.2474\nEpoch [3/5],  Loss: 0.1196\nEpoch [3/5],  Loss: 0.2544\nEpoch [3/5],  Loss: 0.2700\nEpoch [3/5],  Loss: 0.1600\nEpoch [3/5],  Loss: 0.2502\nEpoch [3/5],  Loss: 0.2900\nEpoch [3/5],  Loss: 0.1749\nEpoch [3/5],  Loss: 0.1860\nEpoch [3/5],  Loss: 0.2021\nEpoch [3/5],  Loss: 0.2268\nEpoch [3/5],  Loss: 0.4164\nEpoch [3/5],  Loss: 0.2148\nEpoch [3/5],  Loss: 0.1802\nEpoch [3/5],  Loss: 0.1675\nEpoch [3/5],  Loss: 0.1333\nEpoch [3/5],  Loss: 0.1320\nEpoch [3/5],  Loss: 0.1227\nEpoch [3/5],  Loss: 0.2094\nEpoch [3/5],  Loss: 0.3615\nEpoch [3/5],  Loss: 0.0747\nEpoch [3/5],  Loss: 0.1094\nEpoch [3/5],  Loss: 0.0950\nEpoch [3/5],  Loss: 0.3225\nEpoch [3/5],  Loss: 0.2612\nEpoch [3/5],  Loss: 0.1382\nEpoch [3/5],  Loss: 0.1105\nEpoch [3/5],  Loss: 0.1660\nEpoch [3/5],  Loss: 0.0328\nEpoch [3/5],  Loss: 0.1345\nEpoch [3/5],  Loss: 0.2103\nEpoch [3/5],  Loss: 0.2194\nEpoch [3/5],  Loss: 0.1883\nEpoch [3/5],  Loss: 0.1684\nEpoch [4/5],  Loss: 0.4597\nEpoch [4/5],  Loss: 0.0913\nEpoch [4/5],  Loss: 0.2729\nEpoch [4/5],  Loss: 0.1883\nEpoch [4/5],  Loss: 0.1209\nEpoch [4/5],  Loss: 0.0519\nEpoch [4/5],  Loss: 0.2813\nEpoch [4/5],  Loss: 0.1675\nEpoch [4/5],  Loss: 0.3953\nEpoch [4/5],  Loss: 0.2393\nEpoch [4/5],  Loss: 0.2689\nEpoch [4/5],  Loss: 0.1341\nEpoch [4/5],  Loss: 0.0914\nEpoch [4/5],  Loss: 0.1558\nEpoch [4/5],  Loss: 0.1642\nEpoch [4/5],  Loss: 0.2498\nEpoch [4/5],  Loss: 0.1870\nEpoch [4/5],  Loss: 0.2872\nEpoch [4/5],  Loss: 0.1373\nEpoch [4/5],  Loss: 0.4168\nEpoch [4/5],  Loss: 0.1085\nEpoch [4/5],  Loss: 0.1199\nEpoch [4/5],  Loss: 0.1171\nEpoch [4/5],  Loss: 0.2208\nEpoch [4/5],  Loss: 0.1796\nEpoch [4/5],  Loss: 0.2470\nEpoch [4/5],  Loss: 0.1318\nEpoch [4/5],  Loss: 0.2000\nEpoch [4/5],  Loss: 0.0877\nEpoch [4/5],  Loss: 0.0527\nEpoch [4/5],  Loss: 0.1343\nEpoch [4/5],  Loss: 0.1069\nEpoch [4/5],  Loss: 0.2167\nEpoch [4/5],  Loss: 0.0999\nEpoch [4/5],  Loss: 0.4187\nEpoch [4/5],  Loss: 0.1522\nEpoch [4/5],  Loss: 0.2906\nEpoch [4/5],  Loss: 0.0972\nEpoch [4/5],  Loss: 0.2139\nEpoch [4/5],  Loss: 0.2680\nEpoch [4/5],  Loss: 0.1903\nEpoch [5/5],  Loss: 0.0680\nEpoch [5/5],  Loss: 0.1360\nEpoch [5/5],  Loss: 0.1517\nEpoch [5/5],  Loss: 0.1584\nEpoch [5/5],  Loss: 0.1426\nEpoch [5/5],  Loss: 0.3695\nEpoch [5/5],  Loss: 0.2036\nEpoch [5/5],  Loss: 0.1647\nEpoch [5/5],  Loss: 0.2529\nEpoch [5/5],  Loss: 0.1490\nEpoch [5/5],  Loss: 0.2145\nEpoch [5/5],  Loss: 0.0968\nEpoch [5/5],  Loss: 0.1364\nEpoch [5/5],  Loss: 0.1793\nEpoch [5/5],  Loss: 0.3034\nEpoch [5/5],  Loss: 0.0833\nEpoch [5/5],  Loss: 0.2059\nEpoch [5/5],  Loss: 0.1453\nEpoch [5/5],  Loss: 0.1107\nEpoch [5/5],  Loss: 0.0371\nEpoch [5/5],  Loss: 0.0884\nEpoch [5/5],  Loss: 0.1516\nEpoch [5/5],  Loss: 0.1051\nEpoch [5/5],  Loss: 0.0646\nEpoch [5/5],  Loss: 0.1979\nEpoch [5/5],  Loss: 0.2496\nEpoch [5/5],  Loss: 0.1047\nEpoch [5/5],  Loss: 0.1800\nEpoch [5/5],  Loss: 0.1570\nEpoch [5/5],  Loss: 0.2011\nEpoch [5/5],  Loss: 0.0846\nEpoch [5/5],  Loss: 0.0841\nEpoch [5/5],  Loss: 0.2550\nEpoch [5/5],  Loss: 0.1318\nEpoch [5/5],  Loss: 0.1039\nEpoch [5/5],  Loss: 0.0269\nEpoch [5/5],  Loss: 0.0937\nEpoch [5/5],  Loss: 0.1799\nEpoch [5/5],  Loss: 0.1304\nEpoch [5/5],  Loss: 0.0168\nEpoch [5/5],  Loss: 0.1165\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":" # 保存模型\ntorch.save(model.state_dict(), 'douban_comments_classifier.pth')\n    # 保存模型词典\ntorch.save(vocab, 'douban_comments_vocab.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-17T07:58:07.269059Z","iopub.execute_input":"2025-04-17T07:58:07.269630Z","iopub.status.idle":"2025-04-17T07:58:07.782490Z","shell.execute_reply.started":"2025-04-17T07:58:07.269606Z","shell.execute_reply":"2025-04-17T07:58:07.781714Z"}},"outputs":[],"execution_count":6}]}