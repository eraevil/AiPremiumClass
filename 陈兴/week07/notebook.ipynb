{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-16T07:10:34.718241Z",
     "iopub.status.busy": "2025-04-16T07:10:34.717939Z",
     "iopub.status.idle": "2025-04-16T07:10:36.435243Z",
     "shell.execute_reply": "2025-04-16T07:10:36.434050Z",
     "shell.execute_reply.started": "2025-04-16T07:10:34.718212Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/doubanmovieshortcomments/DMSC.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        comment_file = os.path.join(dirname, filename)\n",
    "        print(comment_file)\n",
    "        df = pd.read_csv(comment_file)\n",
    "        \n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:12:14.203991Z",
     "iopub.status.busy": "2025-04-18T16:12:14.203818Z",
     "iopub.status.idle": "2025-04-18T16:22:36.726257Z",
     "shell.execute_reply": "2025-04-18T16:22:36.725461Z",
     "shell.execute_reply.started": "2025-04-18T16:12:14.203974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/doubanmovieshortcomments/DMSC.csv\n",
      "   ID           Movie_Name_EN Movie_Name_CN  Crawl_Date  Number Username        Date  Star                                            Comment  Like\n",
      "0   0  Avengers Age of Ultron        复仇者联盟2  2017-01-22       1       然潘  2015-05-13     3                                      连奥创都知道整容要去韩国。  2404\n",
      "1   1  Avengers Age of Ultron        复仇者联盟2  2017-01-22       2    更深的白色  2015-04-24     2   非常失望，剧本完全敷衍了事，主线剧情没突破大家可以理解，可所有的人物都缺乏动机，正邪之间、...  1231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.600 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Comment  Star  label                                             tokens                                          input_ids\n",
      "4                             虽然从头打到尾，但是真的很无聊啊。     2      0         [ , 虽然, 从头, 打到, 尾, ，, 但是, 真的, 很, 无聊, 啊, 。]  [2, 84, 573, 3095, 553, 3, 54, 36, 11, 306, 26...\n",
      "7                                  看腻了这些打来打去的烂片     2      0                        [ , 看腻, 了, 这些, 打来打去, 的, 烂片]  [2, 8026, 6, 712, 3342, 4, 156, 0, 0, 0, 0, 0,...\n",
      "15   什么破烂反派，毫无戏剧冲突能消耗两个多小时生命，还强加爱情戏。脑残片好圈钱倒是真的。     1      0  [ , 什么, 破烂, 反派, ，, 毫无, 戏剧, 冲突, 能, 消耗, 两个, 多, 小...  [2, 52, 8884, 908, 3, 556, 3967, 2510, 58, 104...\n",
      "20                           奥创弱爆了弱爆了弱爆了啊！！！！！！     2      0  [ , 奥创, 弱, 爆, 了, 弱, 爆, 了, 弱, 爆, 了, 啊, ！, ！, ！,...  [2, 2122, 627, 536, 6, 627, 536, 6, 627, 536, ...\n",
      "23    越是一本正经，就是越是无趣。个个死不了，撕了扯了砸了死了的，你也不关心。全程梦游。     2      0  [ , 越是, 一本正经, ，, 就是, 越是, 无趣, 。, 个个, 死不了, ，, 撕,...  [2, 5947, 6139, 3, 35, 5947, 2112, 5, 4634, 11...\n",
      "25                          没有了洛基这个小基仔真是觉得即墨如雪啊     5      1         [ , 没有, 了, 洛基, 这个, 小基仔, 真是, 觉得, 即墨, 如雪, 啊]  [2, 34, 6, 1369, 80, 80844, 111, 40, 80845, 43...\n",
      "34                                     劇情弱，純賣特技     2      0                              [ , 劇情, 弱, ，, 純賣, 特技]  [2, 2875, 627, 3, 80846, 635, 0, 0, 0, 0, 0, 0...\n",
      "37                   一般般，然后为什么澳洲电影院的爆米花是咸的！！！！！     2      0  [ , 一般般, ，, 然后, 为什么, 澳洲, 电影院, 的, 爆米花, 是, 咸, 的,...  [2, 1159, 3, 488, 137, 16927, 101, 4, 312, 9, ...\n",
      "38                           我就问一句：老万是被炸死的老万造吗？     4      1      [ , 我, 就, 问, 一句, ：, 老万是, 被, 炸死, 的, 老万造, 吗, ？]  [2, 8, 19, 1060, 434, 235, 80847, 74, 20492, 4...\n",
      "41                        你们看美队的胸！隔着大半个屋子都能感受到！     4      1  [ , 你们, 看美队, 的, 胸, ！, 隔, 着, 大半个, 屋子, 都, 能, 感受,...  [2, 277, 10229, 4, 1745, 7, 3926, 113, 16538, ...\n",
      "Epoch 1, Loss: 14413.0297\n",
      "Epoch 2, Loss: 14416.8715\n",
      "Epoch 3, Loss: 14418.3665\n",
      "Epoch 4, Loss: 14419.4052\n",
      "Epoch 5, Loss: 14418.3234\n",
      "Test Accuracy: 0.7713\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import jieba\n",
    "import pickle\n",
    "import torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "# 1. 使用豆瓣电影评论数据完成文本分类处理：文本预处理，加载、构建词典。（评论得分1～2 表示negative取值：0，评论得分4～5代表positive取值：1）\n",
    "# https://www.kaggle.com/datasets/utmhikari/doubanmovieshortcomments\n",
    "# 2. 加载处理后文本构建词典、定义模型、训练、评估、测试。\n",
    "# 3. 尝试不同分词工具进行文本分词，观察模型训练结果。\n",
    "\n",
    "# 在线访问:\n",
    "# https://www.kaggle.com/code/mitrecx/notebook\n",
    "\n",
    "comment_file = '/kaggle/input/doubanmovieshortcomments/DMSC.csv'\n",
    "print(comment_file)\n",
    "\n",
    "# ID,Movie_Name_EN,Movie_Name_CN,Crawl_Date,Number,Username,Date,Star,Comment,Like\n",
    "df = pd.read_csv(comment_file)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 200)\n",
    "print(df[0:2])\n",
    "\n",
    "df = df[['Comment', 'Star']].dropna()\n",
    "\n",
    "# 标签处理：3分去掉\n",
    "df = df[df['Star'].isin([1, 2, 4, 5])]\n",
    "# 把Comment小于6的过滤掉\n",
    "df = df[df['Comment'].str.len() > 6]\n",
    "df = df[df['Comment'].str.len() <= 50]\n",
    "# 1~2为negative(0), 4~5 为  positive(1)\n",
    "df['label'] = df['Star'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "\n",
    "# print(df[0:10])\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    # stopwords = set()\n",
    "    # with open('cn_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    #     stopwords = set([line.strip() for line in f])\n",
    "    # words = jieba.lcut(text)\n",
    "    # # 过滤停用词\n",
    "    # filtered_words = [word for word in words if word not in stopwords and len(word) > 1]\n",
    "    return jieba.lcut(text)\n",
    "\n",
    "\n",
    "df['tokens'] = df['Comment'].apply(tokenize)\n",
    "# print(df[0:10])\n",
    "\n",
    "# 构建词典\n",
    "all_tokens = [token for tokens in df['tokens'] for token in tokens]\n",
    "vocab = {'<PAD>': 0, '<UNK>': 1}\n",
    "vocab.update({word: i + 2 for i, (word, _) in enumerate(Counter(all_tokens).most_common(100000))})\n",
    "\n",
    "# Save vocabulary\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "# 将文本转为索引\n",
    "def encode(tokens, max_len=50):\n",
    "    ids = [vocab.get(token, 1) for token in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [0] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "\n",
    "df['input_ids'] = df['tokens'].apply(encode)\n",
    "print(df[0:10])\n",
    "\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = torch.tensor(inputs)\n",
    "        self.labels = torch.tensor(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['input_ids'], df['label'], test_size=0.2)\n",
    "\n",
    "train_dataset = CommentDataset(X_train.tolist(), y_train.tolist())\n",
    "test_dataset = CommentDataset(X_test.tolist(), y_test.tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_size=64):\n",
    "        super().__init__()\n",
    "        # 词嵌入, 可训练\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.rnn(emb)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TextClassifier(len(vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Save model\n",
    "torch.save(model.state_dict(), 'text_classifier.pth')\n",
    "\n",
    "# Evaluate model using test set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        outputs = model(batch_x)\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "print(f\"Test Accuracy: {correct / total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T16:37:59.474673Z",
     "iopub.status.busy": "2025-04-18T16:37:59.474413Z",
     "iopub.status.idle": "2025-04-18T16:37:59.703967Z",
     "shell.execute_reply": "2025-04-18T16:37:59.703193Z",
     "shell.execute_reply.started": "2025-04-18T16:37:59.474654Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入：这部电影真心不错，演技在线，剧情感人\n",
      "预测结果：正面评价\n",
      "输入：非常失望，剧本完全敷衍了事\n",
      "预测结果：正面评价\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3531659847.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/kaggle/working/text_classifier.pth', map_location=device))\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import jieba\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "with open('/kaggle/working/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        out, _ = self.rnn(emb)\n",
    "        return self.fc(out[:, -1, :])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = TextClassifier(len(vocab)).to(device)\n",
    "model.load_state_dict(torch.load('/kaggle/working/text_classifier.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "def encode(tokens, vocab, max_len=50):\n",
    "    ids = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    if len(ids) < max_len:\n",
    "        ids += [vocab['<PAD>']] * (max_len - len(ids))\n",
    "    return ids[:max_len]\n",
    "\n",
    "def predict(text):\n",
    "    tokens = jieba.lcut(text)\n",
    "    input_ids = encode(tokens, vocab)\n",
    "    input_tensor = torch.tensor([input_ids]).to(device)  # [1, seq_len]\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        predicted = torch.argmax(probs, dim=1).item()\n",
    "    label_str = \"正面评价\" if predicted == 1 else \"负面评价\"\n",
    "    print(f\"输入：{text}\")\n",
    "    print(f\"预测结果：{label_str}\")\n",
    "\n",
    "predict(\"这部电影真心不错，演技在线，剧情感人\")\n",
    "predict(\"非常失望，剧本完全敷衍了事\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 600,
     "sourceId": 1683,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
