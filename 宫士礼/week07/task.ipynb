{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task01 1. 使用豆瓣电影评论数据完成文本分类处理：文本预处理，加载、构建词典。（评论得分1～2\t表示positive取值：1，评论得分4～5代表negative取值：0）\n",
    "### https://www.kaggle.com/datasets/utmhikari/doubanmovieshortcomments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修复后的存盘文件\n",
    "fixed = open(\"fixed_comments.csv\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "# 修复前的内容文件\n",
    "#读取\n",
    "lines = [line for line in open(r\"E:\\code\\VS_code\\BADOU_AIsystem_file\\week07\\task\\DMSC.csv\", \"r\", encoding=\"utf-8\")]\n",
    "#读取\n",
    "\n",
    "for i ,line in enumerate(lines):#i为索引line为内容\n",
    "    #enumerate() 是 Python 的一个内置函数，用于在遍历可迭代对象（如列表、字符串、文件行等）时，\n",
    "    #同时获取元素的索引（序号）和元素本身\n",
    "    #保存标题列\n",
    "    if i == 0:\n",
    "        fixed.write(line)\n",
    "        pre_line = ''#上一行的书名\n",
    "        continue\n",
    "    #提取书名和评论文本\n",
    "    terms = line.split(',')\n",
    "\n",
    "    #是否属于同一本书\n",
    "    if len(terms) != 10: continue\n",
    "    if pre_line!= '' and terms[2] == pre_line.split(',')[2]:\n",
    "        if len(pre_line.split(\",\")) == 10:#上一行的内容是否是一个完整的记录\n",
    "            #保存上一行的记录\n",
    "            fixed.write(pre_line + '\\n')\n",
    "            pre_line = line.strip()#去除当前行 (line) 的首尾空白字符（包括换行符 \\n 和空格等） #，并将处理后的字符串保存到 pre_line 中\n",
    "            \n",
    "        else:\n",
    "            pre_line =\"\"\n",
    "    #是否属于不同本书\n",
    "    else:  \n",
    "        fixed.write(line)\n",
    "        pre_line = line.strip()#保存当前行\n",
    "        \n",
    "\n",
    "            \n",
    "            \n",
    "        \n",
    "fixed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50001\n"
     ]
    }
   ],
   "source": [
    "## 评论得分列表的构建\n",
    "##所获得的列表为一个包含元组的列表\n",
    "import csv\n",
    "import jieba\n",
    "import pickle\n",
    "#用户评论集\n",
    "ds_coments = []\n",
    "with open(r'E:\\code\\VS_code\\BADOU_AIsystem_file\\week07\\task\\fixed_comments.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.DictReader(f)#使用 csv.DictReader（适用于有表头的 CSV）\n",
    "    for row in reader:\n",
    "        if len(ds_coments)>50000:#五万条数据\n",
    "            break\n",
    "        vote =int(row['Star'])\n",
    "        \n",
    "        if vote in [1,2,4,5]:\n",
    "            words =jieba.lcut(row['Comment'])\n",
    "            ds_coments.append((words, 0 if vote > 0 and vote < 3 else 1)) \n",
    "ds_commnts = [c for c in ds_coments if len(c[0]) in range(1, 150)]\n",
    "\n",
    "\n",
    "with open(r'E:\\code\\VS_code\\BADOU_AIsystem_file\\week07\\task\\score_list.pkl', 'wb') as f:\n",
    "    pickle.dump(ds_commnts, f)\n",
    "print(len(ds_coments) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([' ', '承认', '这货', '很烂', '很难', '吗', '？', '混乱', '的', '节奏', '，', '人人', '都', '可能', '冒出来', '即兴', '插一腿', '，', '糟糕', '的', '3D', '也', '是', '毫无', '必要', '，', '就', '连', '特效', '大', '场面', '都', '没有', '什么', '令人', '惊艳', '的', '地方', '，', '其实', '这些', '都', '不', '重要', '了', '，', '我', '只', '想', '知道', '一点', '，', '黑寡妇', '下', '一部', '的', '时候', '会', '跟', '哪位', '队友', '搞', '暧昧', '？', '这种', '“', '白痴', '专供', '”', '片子', '如今', '算是', '彻底', '找到', '自己', '的', '死忠', '了', '，', '漫威', '可以', '继续', '一招', '鲜吃遍', '天', '了', '。', '★', '★'], 0)\n"
     ]
    }
   ],
   "source": [
    "print(ds_coments[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据+构建词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分词字典的构建\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence  # 长度不同张量填充为相同长度\n",
    "import jieba\n",
    "import csv\n",
    "def build_from_doc(doc):\n",
    "    vocab = set()\n",
    "    for line in doc:\n",
    "        vocab.update(line[0])\n",
    "\n",
    "    vocab =  ['PAD','UNK'] + list(vocab)  # PAD: padding, UNK: unknown\n",
    "    w2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return w2idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task02加载处理后文本构建词典、定义模型、训练、评估、测试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义模型\n",
    "class Comments_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # padding_idx=0\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (batch_size, seq_len)\n",
    "        # embedded: (batch_size, seq_len, embedding_dim)\n",
    "        embedded = self.embedding(input_ids)\n",
    "        # output: (batch_size, seq_len, hidden_size)\n",
    "        output, (hidden, _) = self.rnn(embedded)\n",
    "        output = self.fc(output[:, -1, :])  # 取最后一个时间步的输出\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 46039\n",
      "Epoch [1/1], Step [100/12501], Loss: 0.5558\n",
      "Epoch [1/1], Step [200/12501], Loss: 0.2539\n",
      "Epoch [1/1], Step [300/12501], Loss: 0.5538\n",
      "Epoch [1/1], Step [400/12501], Loss: 0.5475\n",
      "Epoch [1/1], Step [500/12501], Loss: 0.6152\n",
      "Epoch [1/1], Step [600/12501], Loss: 0.3533\n",
      "Epoch [1/1], Step [700/12501], Loss: 0.5667\n",
      "Epoch [1/1], Step [800/12501], Loss: 0.5232\n",
      "Epoch [1/1], Step [900/12501], Loss: 0.5347\n",
      "Epoch [1/1], Step [1000/12501], Loss: 0.6175\n",
      "Epoch [1/1], Step [1100/12501], Loss: 0.8888\n",
      "Epoch [1/1], Step [1200/12501], Loss: 0.5534\n",
      "Epoch [1/1], Step [1300/12501], Loss: 1.2594\n",
      "Epoch [1/1], Step [1400/12501], Loss: 0.5404\n",
      "Epoch [1/1], Step [1500/12501], Loss: 1.1500\n",
      "Epoch [1/1], Step [1600/12501], Loss: 0.4042\n",
      "Epoch [1/1], Step [1700/12501], Loss: 0.3425\n",
      "Epoch [1/1], Step [1800/12501], Loss: 0.2997\n",
      "Epoch [1/1], Step [1900/12501], Loss: 0.8031\n",
      "Epoch [1/1], Step [2000/12501], Loss: 0.3362\n",
      "Epoch [1/1], Step [2100/12501], Loss: 0.9432\n",
      "Epoch [1/1], Step [2200/12501], Loss: 0.5402\n",
      "Epoch [1/1], Step [2300/12501], Loss: 0.5604\n",
      "Epoch [1/1], Step [2400/12501], Loss: 0.4055\n",
      "Epoch [1/1], Step [2500/12501], Loss: 0.2827\n",
      "Epoch [1/1], Step [2600/12501], Loss: 0.3037\n",
      "Epoch [1/1], Step [2700/12501], Loss: 0.4615\n",
      "Epoch [1/1], Step [2800/12501], Loss: 0.5561\n",
      "Epoch [1/1], Step [2900/12501], Loss: 0.8106\n",
      "Epoch [1/1], Step [3000/12501], Loss: 0.3450\n",
      "Epoch [1/1], Step [3100/12501], Loss: 0.5275\n",
      "Epoch [1/1], Step [3200/12501], Loss: 0.2209\n",
      "Epoch [1/1], Step [3300/12501], Loss: 0.2818\n",
      "Epoch [1/1], Step [3400/12501], Loss: 0.3393\n",
      "Epoch [1/1], Step [3500/12501], Loss: 0.6446\n",
      "Epoch [1/1], Step [3600/12501], Loss: 0.2777\n",
      "Epoch [1/1], Step [3700/12501], Loss: 0.6144\n",
      "Epoch [1/1], Step [3800/12501], Loss: 0.7941\n",
      "Epoch [1/1], Step [3900/12501], Loss: 0.2693\n",
      "Epoch [1/1], Step [4000/12501], Loss: 0.9234\n",
      "Epoch [1/1], Step [4100/12501], Loss: 0.5532\n",
      "Epoch [1/1], Step [4200/12501], Loss: 0.3720\n",
      "Epoch [1/1], Step [4300/12501], Loss: 0.5523\n",
      "Epoch [1/1], Step [4400/12501], Loss: 0.5268\n",
      "Epoch [1/1], Step [4500/12501], Loss: 0.5660\n",
      "Epoch [1/1], Step [4600/12501], Loss: 0.4405\n",
      "Epoch [1/1], Step [4700/12501], Loss: 0.7578\n",
      "Epoch [1/1], Step [4800/12501], Loss: 0.5762\n",
      "Epoch [1/1], Step [4900/12501], Loss: 0.5191\n",
      "Epoch [1/1], Step [5000/12501], Loss: 0.5625\n",
      "Epoch [1/1], Step [5100/12501], Loss: 0.5767\n",
      "Epoch [1/1], Step [5200/12501], Loss: 0.8363\n",
      "Epoch [1/1], Step [5300/12501], Loss: 0.5117\n",
      "Epoch [1/1], Step [5400/12501], Loss: 0.4952\n",
      "Epoch [1/1], Step [5500/12501], Loss: 0.7756\n",
      "Epoch [1/1], Step [5600/12501], Loss: 0.7683\n",
      "Epoch [1/1], Step [5700/12501], Loss: 0.5549\n",
      "Epoch [1/1], Step [5800/12501], Loss: 0.2232\n",
      "Epoch [1/1], Step [5900/12501], Loss: 0.5302\n",
      "Epoch [1/1], Step [6000/12501], Loss: 0.5544\n",
      "Epoch [1/1], Step [6100/12501], Loss: 0.5672\n",
      "Epoch [1/1], Step [6200/12501], Loss: 0.7038\n",
      "Epoch [1/1], Step [6300/12501], Loss: 0.6702\n",
      "Epoch [1/1], Step [6400/12501], Loss: 0.5526\n",
      "Epoch [1/1], Step [6500/12501], Loss: 1.1307\n",
      "Epoch [1/1], Step [6600/12501], Loss: 0.3076\n",
      "Epoch [1/1], Step [6700/12501], Loss: 0.2528\n",
      "Epoch [1/1], Step [6800/12501], Loss: 0.5530\n",
      "Epoch [1/1], Step [6900/12501], Loss: 0.5815\n",
      "Epoch [1/1], Step [7000/12501], Loss: 0.3063\n",
      "Epoch [1/1], Step [7100/12501], Loss: 0.4591\n",
      "Epoch [1/1], Step [7200/12501], Loss: 0.6929\n",
      "Epoch [1/1], Step [7300/12501], Loss: 0.6605\n",
      "Epoch [1/1], Step [7400/12501], Loss: 0.9243\n",
      "Epoch [1/1], Step [7500/12501], Loss: 0.1069\n",
      "Epoch [1/1], Step [7600/12501], Loss: 0.6320\n",
      "Epoch [1/1], Step [7700/12501], Loss: 0.3189\n",
      "Epoch [1/1], Step [7800/12501], Loss: 0.1874\n",
      "Epoch [1/1], Step [7900/12501], Loss: 0.2663\n",
      "Epoch [1/1], Step [8000/12501], Loss: 0.1284\n",
      "Epoch [1/1], Step [8100/12501], Loss: 0.3899\n",
      "Epoch [1/1], Step [8200/12501], Loss: 0.5665\n",
      "Epoch [1/1], Step [8300/12501], Loss: 0.3998\n",
      "Epoch [1/1], Step [8400/12501], Loss: 0.2758\n",
      "Epoch [1/1], Step [8500/12501], Loss: 0.5344\n",
      "Epoch [1/1], Step [8600/12501], Loss: 0.1439\n",
      "Epoch [1/1], Step [8700/12501], Loss: 0.4184\n",
      "Epoch [1/1], Step [8800/12501], Loss: 0.2962\n",
      "Epoch [1/1], Step [8900/12501], Loss: 0.5260\n",
      "Epoch [1/1], Step [9000/12501], Loss: 0.2650\n",
      "Epoch [1/1], Step [9100/12501], Loss: 0.3998\n",
      "Epoch [1/1], Step [9200/12501], Loss: 0.6415\n",
      "Epoch [1/1], Step [9300/12501], Loss: 0.1997\n",
      "Epoch [1/1], Step [9400/12501], Loss: 0.5285\n",
      "Epoch [1/1], Step [9500/12501], Loss: 0.4968\n",
      "Epoch [1/1], Step [9600/12501], Loss: 0.7974\n",
      "Epoch [1/1], Step [9700/12501], Loss: 0.9140\n",
      "Epoch [1/1], Step [9800/12501], Loss: 0.3309\n",
      "Epoch [1/1], Step [9900/12501], Loss: 0.5623\n",
      "Epoch [1/1], Step [10000/12501], Loss: 0.7174\n",
      "Epoch [1/1], Step [10100/12501], Loss: 0.2655\n",
      "Epoch [1/1], Step [10200/12501], Loss: 0.6615\n",
      "Epoch [1/1], Step [10300/12501], Loss: 0.3495\n",
      "Epoch [1/1], Step [10400/12501], Loss: 0.3061\n",
      "Epoch [1/1], Step [10500/12501], Loss: 0.6370\n",
      "Epoch [1/1], Step [10600/12501], Loss: 0.2333\n",
      "Epoch [1/1], Step [10700/12501], Loss: 0.5367\n",
      "Epoch [1/1], Step [10800/12501], Loss: 0.3355\n",
      "Epoch [1/1], Step [10900/12501], Loss: 0.8265\n",
      "Epoch [1/1], Step [11000/12501], Loss: 0.3089\n",
      "Epoch [1/1], Step [11100/12501], Loss: 0.4598\n",
      "Epoch [1/1], Step [11200/12501], Loss: 0.2808\n",
      "Epoch [1/1], Step [11300/12501], Loss: 0.5490\n",
      "Epoch [1/1], Step [11400/12501], Loss: 0.2839\n",
      "Epoch [1/1], Step [11500/12501], Loss: 0.5293\n",
      "Epoch [1/1], Step [11600/12501], Loss: 0.4815\n",
      "Epoch [1/1], Step [11700/12501], Loss: 0.1389\n",
      "Epoch [1/1], Step [11800/12501], Loss: 0.2347\n",
      "Epoch [1/1], Step [11900/12501], Loss: 0.5419\n",
      "Epoch [1/1], Step [12000/12501], Loss: 0.2414\n",
      "Epoch [1/1], Step [12100/12501], Loss: 0.5402\n",
      "Epoch [1/1], Step [12200/12501], Loss: 0.3750\n",
      "Epoch [1/1], Step [12300/12501], Loss: 0.6489\n",
      "Epoch [1/1], Step [12400/12501], Loss: 0.6214\n",
      "Epoch [1/1], Step [12500/12501], Loss: 0.1752\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载训练语料\n",
    "    with open(r'E:\\code\\VS_code\\BADOU_AIsystem_file\\week07\\task\\score_list.pkl','rb') as f:\n",
    "        comments_data = pickle.load(f)\n",
    "\n",
    "    # 构建词汇表\n",
    "    vocab = build_from_doc(comments_data)\n",
    "    print('词汇表大小:', len(vocab))\n",
    "\n",
    "    # 所有向量集合 Embedding（词嵌入）\n",
    "    emb = nn.Embedding(len(vocab), 100) # 词汇表大小，向量维度\n",
    "\n",
    "    # 自定义数据转换方法(callback function)回调函数\n",
    "    # 该函数会在每个batch数据加载时被调用\n",
    "    def convert_data(batch_data):\n",
    "        comments, votes = [],[]\n",
    "        # 分别提取评论和标签\n",
    "        for comment, vote in batch_data:\n",
    "            comments.append(torch.tensor([vocab.get(word, vocab['UNK']) for word in comment]))\n",
    "            votes.append(vote)\n",
    "        \n",
    "        # 将评论和标签转换为tensor\n",
    "        commt = pad_sequence(comments, batch_first=True, padding_value=vocab['PAD'])  # 填充为相同长度\n",
    "        labels = torch.tensor(votes)\n",
    "        # 返回评论和标签\n",
    "        return commt, labels\n",
    "\n",
    "    # 通过Dataset构建DataLoader\n",
    "    dataloader = DataLoader(comments_data, batch_size=4, shuffle=True, \n",
    "                            collate_fn=convert_data)#collate_fn: 自定义数据转换方法\n",
    "\n",
    "    # 构建模型\n",
    "    # vocab_size: 词汇表大小\n",
    "    # embedding_dim: 词嵌入维度\n",
    "    # hidden_size: LSTM隐藏层大小\n",
    "    # num_classes: 分类数量\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 100\n",
    "    hidden_size = 128\n",
    "    num_classes = 2\n",
    "\n",
    "    model = Comments_Classifier(len(vocab), embedding_dim, hidden_size, num_classes)\n",
    "    model\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # 训练模型\n",
    "    num_epochs = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (cmt, lbl) in enumerate(dataloader):\n",
    "            cmt = cmt\n",
    "            lbl = lbl\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(cmt)\n",
    "            loss = criterion(outputs, lbl)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'comments_classifier_se.pth')\n",
    "    # 模型词典\n",
    "    torch.save(vocab, 'comments_vocab_se.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论1预测结果: 1\n",
      "评论2预测结果: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\阿尼马\\AppData\\Local\\Temp\\ipykernel_896\\2934440317.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vocab = torch.load('comments_vocab.pth')\n",
      "C:\\Users\\阿尼马\\AppData\\Local\\Temp\\ipykernel_896\\2934440317.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('comments_classifier.pth'))\n"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "# 加载词典\n",
    "vocab = torch.load('comments_vocab.pth')\n",
    "# 测试模型\n",
    "comment1 = '这部电影很好看，全程无尿点'\n",
    "comment2 = '没见过这样的烂片，真垃圾'\n",
    "\n",
    "# 将评论转换为索引\n",
    "comment1_idx = torch.tensor([vocab.get(word, vocab['UNK']) for word in jieba.lcut(comment1)])\n",
    "comment2_idx = torch.tensor([vocab.get(word, vocab['UNK']) for word in jieba.lcut(comment2)])\n",
    "# 将评论转换为tensor\n",
    "comment1_idx = comment1_idx.unsqueeze(0).to(device)  # 添加batch维度    \n",
    "comment2_idx = comment2_idx.unsqueeze(0).to(device)  # 添加batch维度\n",
    "\n",
    "# 加载模型\n",
    "model = Comments_Classifier(len(vocab), embedding_dim, hidden_size, num_classes)\n",
    "model.load_state_dict(torch.load('comments_classifier.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# 模型推理\n",
    "pred1 = model(comment1_idx)\n",
    "pred2 = model(comment2_idx)\n",
    "\n",
    "# 取最大值的索引作为预测结果\n",
    "pred1 = torch.argmax(pred1, dim=1).item()\n",
    "pred2 = torch.argmax(pred2, dim=1).item()\n",
    "print(f'评论1预测结果: {pred1}')\n",
    "print(f'评论2预测结果: {pred2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task3. 尝试不同分词工具进行文本分词，观察模型训练结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据处理为sentence可接受的形式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 10000\n",
      "ID 0: <unk>\n",
      "ID 1: <s>\n",
      "ID 2: </s>\n",
      "ID 3: ..\n",
      "ID 4: 。。\n",
      "ID 5: 电影\n",
      "ID 6: 剧情\n",
      "ID 7: ....\n",
      "ID 8: 没有\n",
      "ID 9: 一部\n",
      "\n",
      "ID 1对应的token: <s>\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# 训练模型\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input=r'/kaggle/working/fixed1_comments.txt',\n",
    "    model_prefix='m',\n",
    "    vocab_size=10000,\n",
    "    character_coverage=1.0,\n",
    "    model_type='bpe'\n",
    ")\n",
    "\n",
    "# 加载模型并构建字典\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "vocab_dict = {sp.id_to_piece(i): i for i in range(sp.get_piece_size())}\n",
    "\n",
    "# 保存字典\n",
    "import pickle\n",
    "with open('vocab_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_dict, f)\n",
    "\n",
    "# 打印词汇表大小和前10个token\n",
    "print(f\"词汇表大小: {len(vocab_dict)}\")\n",
    "print(\"示例token:\")\n",
    "for token, idx in list(vocab_dict.items())[:10]:\n",
    "    print(f\"{idx}: {token}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "词汇表大小: 10000\n",
      "ID 0: <unk>\n",
      "ID 1: <s>\n",
      "ID 2: </s>\n",
      "ID 3: ..\n",
      "ID 4: 。。\n",
      "ID 5: 电影\n",
      "ID 6: 剧情\n",
      "ID 7: ....\n",
      "ID 8: 没有\n",
      "ID 9: 一部\n",
      "\n",
      "ID 1对应的token: <s>\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # 加载训练语料\n",
    "    with open(r'/kaggle/working/score_list.pkl','rb') as f:\n",
    "        comments_data = pickle.load(f)\n",
    "\n",
    "    # 构建词汇表\n",
    "    with open(r'/kaggle/working/vocab_dict.pkl','rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # 所有向量集合 Embedding（词嵌入）\n",
    "    emb = nn.Embedding(len(vocab), 100) # 词汇表大小，向量维度\n",
    "\n",
    "    # 自定义数据转换方法(callback function)回调函数\n",
    "    # 该函数会在每个batch数据加载时被调用\n",
    "    def convert_data(batch_data):\n",
    "        comments, votes = [],[]\n",
    "        # 分别提取评论和标签\n",
    "        for comment, vote in batch_data:\n",
    "            comments.append(torch.tensor([vocab.get(word, vocab['<unk>']) for word in comment]))\n",
    "            votes.append(vote)\n",
    "        \n",
    "        # 将评论和标签转换为tensor\n",
    "        commt = pad_sequence(comments, batch_first=True, padding_value=vocab['<s>'])  # 填充为相同长度\n",
    "        labels = torch.tensor(votes)\n",
    "        # 返回评论和标签\n",
    "        return commt, labels\n",
    "\n",
    "    # 通过Dataset构建DataLoader\n",
    "    dataloader = DataLoader(comments_data, batch_size=4, shuffle=True, \n",
    "                            collate_fn=convert_data)#collate_fn: 自定义数据转换方法\n",
    "\n",
    "    # 构建模型\n",
    "    # vocab_size: 词汇表大小\n",
    "    # embedding_dim: 词嵌入维度\n",
    "    # hidden_size: LSTM隐藏层大小\n",
    "    # num_classes: 分类数量\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 100\n",
    "    hidden_size = 128\n",
    "    num_classes = 2\n",
    "\n",
    "    model = Comments_Classifier(len(vocab), embedding_dim, hidden_size, num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # 训练模型\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (cmt, lbl) in enumerate(dataloader):\n",
    "            cmt = cmt.to(device)\n",
    "            lbl = lbl.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(cmt)\n",
    "            loss = criterion(outputs, lbl)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (i+1) % 2000 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), 'comments_classifier.pth')\n",
    "    # 模型词典\n",
    "    torch.save(vocab, 'comments_vocab.pth')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练过程\n",
    "\n",
    "![alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试模型\n",
    "embedding_dim = 100\n",
    "hidden_size = 128\n",
    "num_classes = 2\n",
    "\n",
    "# 加载词典\n",
    "vocab = torch.load('comments_vocab.pth')\n",
    "# 测试模型\n",
    "comment1 = '这部电影很好看，全程无尿点'\n",
    "comment2 = '真牛，第一次感觉到不好看是什么滋味'\n",
    "\n",
    "# 将评论转换为索引\n",
    "comment1_idx = torch.tensor([vocab.get(word, vocab['<unk>']) for word in jieba.lcut(comment1)])\n",
    "comment2_idx = torch.tensor([vocab.get(word, vocab['<unk>']) for word in jieba.lcut(comment2)])\n",
    "# 将评论转换为tensor\n",
    "comment1_idx = comment1_idx.unsqueeze(0).to(device)  # 添加batch维度    \n",
    "comment2_idx = comment2_idx.unsqueeze(0).to(device)  # 添加batch维度\n",
    "\n",
    "# 加载模型\n",
    "model = Comments_Classifier(len(vocab), embedding_dim, hidden_size, num_classes)\n",
    "model.load_state_dict(torch.load('comments_classifier.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# 模型推理\n",
    "pred1 = model(comment1_idx)\n",
    "pred2 = model(comment2_idx)\n",
    "\n",
    "# 取最大值的索引作为预测结果\n",
    "pred1 = torch.argmax(pred1, dim=1).item()\n",
    "pred2 = torch.argmax(pred2, dim=1).item()\n",
    "print(f'评论1预测结果: {pred1}')\n",
    "print(f'评论2预测结果: {pred2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试结果\n",
    "![alt text](image.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
