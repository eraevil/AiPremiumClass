# base版BERT

词向量长度 = 768

词表数量 = 30522
词向量embedding编码 = 词表数量 * 词向量长度

最大输入长度 = 512
位置embedding编码 = 最大输入长度 * 词向量长度

任务输入句子数量 = 2
断句embedding编码 = 任务输入句子数量 * 词向量长度

线性层归一化参数 = 2
归一化层参数 = 线性层归一化参数 * 词向量长度

目前参数总数 = 词向量embedding编码 + 位置embedding编码 + 断句embedding编码 + 归一化层参数


注意力三个矩阵和偏置 = 3 * (词向量长度 * 词向量长度 + 词向量长度)
注意力输出映射线性层 = 词向量长度 * 词向量长度 + 词向量长度
归一化层参数 = 归一化层参数
单层注意力 = 注意力三个矩阵和偏置 + 注意力输出映射线性层 + 归一化层参数

单层前馈 = (词向量长度 * (4*词向量长度) + 4*词向量长度) + ((4*词向量长度) * 词向量长度 + 词向量长度) + 归一化层参数

单层编码器 = 单层注意力 + 单层前馈

目前参数总数 = 目前参数总数 + 单层编码器 * 12

# 任务头
Pooler层对CLS一个token线性变换 = 词向量长度 * 词向量长度 + 词向量长度
NSP任务头预测两句话是否连贯二分类 = 词向量长度 * 2 + 2

MLM预测被mask掉的词 = 0 # 不知道咋算

目前参数总数 = 目前参数总数 + Pooler层对CLS一个token线性变换 + NSP任务头预测两句话是否连贯二分类 + MLM预测被mask掉的词

print(目前参数总数)